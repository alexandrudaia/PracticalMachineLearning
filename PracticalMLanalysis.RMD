#ML PROJECT
#READING  THE DATA
library(caret)

library(ISLR)

library(kernlab)

setwd("C:/Users/Home/Desktop/mlProject")

data=read.table("pml-training.csv",header=F,sep=",",skip=1)

cnames=readLines("pml-training.csv",1)

cnames=strsplit(cnames,",",fixed=T)

cnames[[1]][1]="id"

cnames[[1]][2]=gsub("\"","",cnames[[1]][2])

cnames[[1]][]=gsub("\"","",cnames[[1]][])

names(data)=make.names(cnames[[1]])

#1)exploring nan's and  removing  columns  with number of nan's bigger than   the treshold

treshold=nrow(data)*(1/2.5)
data=data[,colSums(is.na(data))<=treshold]
#2)exploring ""s and  removing  columns  with number of ""s bigger than   the treshold
data=data[,colSums((data==""))<=treshold]
#3)Removing  zero  covariates(with lowest  percent  unique)and  columns  with zero information
nzv=nearZeroVar(data,saveMetrics=T)#one  nzv  value
data=data[,-c(1,2,3,4,5,6,7)]#which(colnames(data)=="new_window")

#4)-data slicing

inTrain=createDataPartition(y=data$classe,p=0.75,list=F)
train=data[inTrain,]
test=data[-inTrain,]
dim(data)
dim(train)
dim(test)
#4') EXPLORING DATA WITH  CLUSTERING 

names(train)[1:13]
table(train$classe)
#one thing  we could do is to  plot  accel belt belt for the  firs  subject carlitos
par(mfrow=c(1,2))
mar=c(5,4,1,1)
train=transform(train,classe=factor(classe))#y converted to  factor
subset=subset(train,user_name=="carlitos")#subset  for  carlitos - attentio to data variable - do not remove  username
plot(subset$accel_belt_x,col=subset$classe,ylab=names(subset$accel_belt_x))
plot(subset$accel_belt_y,col=subset$classe,ylab=names(subset$accel_belt_z))
legend("bottomright",legend=unique(subset$classe),col=unique(subset$classe),pch=1)
                                              #we can see that  there is nu very much  information  regardin  accel_belt except E classe
#CLUSTERING  BASED ON  accel_belt_x,accel_belt_y,accel_belt_Z
source("myplclust.R")
distMatrix=dist(subset[,7:9])
hclustering=hclust(distMatrix)
myplclust(hclustering, lab.col = unclass(subset$classe))#no result
# we can  observ it  is  not very clear
#the accel_belt   features do not appear to be able to discriminate between the 5 different classe's.
#WE CAN   TRY TO PLOT  OTHER   FEATURES FOR  FIRST SUBJECT
#PLOTTING roll_belt and  pitch_belt
par(mfrow=c(1,2))
mar=c(5,4,1,1)
plot(subset$roll_belt,col=subset$classe,ylab=names(subset$roll_belt))
plot(subset$yaw_belt,col=subset$classe,ylab=names(subset$yaw_belt))
legend("topleft",legend=unique(subset$classe),col=unique(subset$classe),pch=1)
                                           # possible cluster  for  C and  D   based on this features
#clustering  bassed on  the  features roll_belt and  pitch_belt
distMatrix=dist(subset[,3:4])
hclustering=hclust(distMatrix)
myplclust(hclustering, lab.col = unclass(subset$classe))#no result
# the objective was   to  separate  this classe's  based on  the predictors

#SINGULAR  VALUE DECOMPOSITION
par(mfrow=c(1,2))
svd1=svd(scale(subset[,-c(1,2,55)]))
plot(svd1$u[,1],col=subset$classe,pch=19)
plot(svd1$u[,2],col=subset$classe,pch=19)
# no results
#new clustering  with  the maximum contributor
maxContrib=which.max(svd1$v[,2])
names(data[,-c(1,2,10)])[maxContrib]#the max contributor but it does not help us
#clustering with  K-means
kClust=kmeans(subset[,-c(53)], centers=10,nstart=10)
table(kClust$cluster,subset$classe)#we  find one cluster
plot(kClust$center[10,1:30],pch=19,ylab="cluster centers",xlab="")

#5)Pre Processing  with PCA

#creating the correlation matrix(to see  what   features are strongly corellated)
#working  on  train  since  we need   numeric  var's
m=abs(cor(train[,-length(colnames(train))]))
diag(m)=0
which(m>0.8,arr.ind=T)# we  saee  that   there are lot  of  correlated  columns
preProcess.default(x = train[,-length(colnames(train))], method = "pca", thresh = 0.90)
                                         #PCA needed 18 components to capture 90 percent of the variance
#6) 
#A-Predictiong  with   glm  not working  only 2 outcomes 

#B-Predicting  with  trees and analyse   accuracy of pca and non pca
modFit=train(classe~.,method="rpart",data=train)#print(modFit$finalModel) n=14718 nodes
pred=predict(modFit,newdata=test)
c=confusionMatrix(pred,test$classe)
#Accuracy : 0.5 
modelFit <- train(train$classe ~ ., method = "rpart", preProcess = "pca", 
                                    data = train, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
pred=predict(modelFit,newdata=test)
c=confusionMatrix(pred,test$classe)
#Accuracy : 0.32

#C) PREDICTING  WITH BAGGING(TREES)->>avg of  the  results  of trees  where  each  tree corresponds  to a subsample
predictors=data.frame(train[,-length(colnames(train))])
y=train$classe
bagtree=bag(predictors,y,B=4,bagControl=bagControl(fit=ctreeBag$fit,predict=ctreeBag$pred,aggregate=ctreeBag$aggregate))
testbag=data.frame(test[,-length(colnames(test))])
p=predict(bagtree,testbag)
c=confusionMatrix(p,test$classe)#accuracy : 0.938 :D
#D)PREDICTING  WITH random forest (EXTENSION OF BAGGING- DIFFERENCE 0 AT EACH  SPLIT BOOTSRAP  VARIABLES)
#attention y must  be factor  otherwise  it will   make  regression
library(randomForest)
modelFit=randomForest(classe~.,data=train,ntree=1000)
p=predict(modelFit,test)

c=confusionMatrix(p,test$classe)# Accuracy : 0.9959 :D

#E) PREDICTING WITH BOOSTING-take a lot of possibly weak predictors weight  them up
# and add  them up and  get a  stronger predictor f(x)=sgn(sum(alpha_i*h_i(x))).
#h1....hk- set of  classifier   that  the alghoritm is starting with(any classifier)
#boosting  with trees#attention   to bee  factor  y in order  to make   classification 
modFit=train(classe~.,method="gbm",data=train,verbose=FALSE)
p=predict(modelFit,test)#sometimes better   accuracy
#F)PREDICTING WITH Ensemble learning- COMBINING PREDICTORS
modelBoosting=train(Survived~.,method="gbm",data=train,verbose=FALSE)
modelRandomFores=train(Survived~.,method="rf",data=train,verbose=FALSE)
pred1=predict(modelBoosting,test)
pred2=predict(modelRandomFores,test)
qplot(pred1,pred2,data=test)#in case  it is regression problem
predDF=data.frame(pred1,pred2,classe=test$classe)
comb=train(classe~.,method="gam",data=predDF)
combPred=predict(comb,predDF)



 

